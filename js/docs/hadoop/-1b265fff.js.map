{"version":3,"sources":["webpack:///webpack/bootstrap","webpack:///./src/pages/md/bigData_hadoop_基础_1-1_HdfsShell.md?fbda","webpack:///./node_modules/vue-loader/lib/runtime/componentNormalizer.js","webpack:///./src/pages/md/bigData_hadoop_基础_1-1_HdfsShell.md","webpack:///./src/pages/md/bigData_hadoop_基础_1-1_HdfsShell.md?8992"],"names":[],"mappings":";;;;;;;QAAA;QACA;;QAEA;QACA;;QAEA;QACA;QACA;QACA;QACA;QACA;QACA;QACA;QACA;QACA;;QAEA;QACA;;QAEA;QACA;;QAEA;QACA;QACA;;;QAGA;QACA;;QAEA;QACA;;QAEA;QACA;QACA;QACA,0CAA0C,gCAAgC;QAC1E;QACA;;QAEA;QACA;QACA;QACA,wDAAwD,kBAAkB;QAC1E;QACA,iDAAiD,cAAc;QAC/D;;QAEA;QACA;QACA;QACA;QACA;QACA;QACA;QACA;QACA;QACA;QACA;QACA,yCAAyC,iCAAiC;QAC1E,gHAAgH,mBAAmB,EAAE;QACrI;QACA;;QAEA;QACA;QACA;QACA,2BAA2B,0BAA0B,EAAE;QACvD,iCAAiC,eAAe;QAChD;QACA;QACA;;QAEA;QACA,sDAAsD,+DAA+D;;QAErH;QACA;;;QAGA;QACA;;;;;;;;;;;;;AClFA;AAAA;AAAA;AAAA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,sBAAsB,yBAAyB;AAC/C,gBAAgB,SAAS,gCAAgC,EAAE;AAC3D;AACA;AACA;AACA;AACA,oBAAoB;AACpB,WAAW;AACX;AACA;AACA;AACA;AACA;AACA;AACA;AACA,iBAAiB,sBAAsB;AACvC;AACA;AACA;AACA;AACA,sBAAsB,6BAA6B;AACnD;AACA;AACA;AACA,sBAAsB,6BAA6B;AACnD;AACA;AACA;AACA,sBAAsB,6BAA6B;AACnD;AACA,sBAAsB,6BAA6B;AACnD;AACA;AACA;AACA;AACA;AACA;AACA,iBAAiB,sCAAsC;AACvD,iBAAiB,oCAAoC;AACrD;AACA,mBAAmB,sBAAsB;AACzC;AACA;AACA,wBAAwB,2BAA2B;AACnD;AACA,wBAAwB,8BAA8B;AACtD;AACA;AACA;AACA,wBAAwB,2BAA2B;AACnD;AACA;AACA;AACA,wBAAwB,4BAA4B;AACpD,0BAA0B,8BAA8B;AACxD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA;AACA;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA;AACA;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA;AACA;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA;AACA;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA;AACA;AACA,0BAA0B,4BAA4B;AACtD;AACA;AACA;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA;AACA;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA,0BAA0B,4BAA4B;AACtD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,iBAAiB,sBAAsB;AACvC;AACA;AACA,sBAAsB,2BAA2B;AACjD;AACA,sBAAsB,2BAA2B;AACjD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,mSAAmS;AACnS;AACA;AACA;AACA;AACA,iBAAiB,sBAAsB;AACvC;AACA;AACA,sBAAsB,2BAA2B;AACjD;AACA,sBAAsB,6BAA6B;AACnD;AACA,sBAAsB,6BAA6B;AACnD;AACA,sBAAsB,6BAA6B;AACnD;AACA,sBAAsB,6BAA6B;AACnD;AACA,sBAAsB,6BAA6B;AACnD;AACA,sBAAsB,6BAA6B;AACnD;AACA,sBAAsB,6BAA6B;AACnD;AACA,sBAAsB,2BAA2B;AACjD;AACA;AACA;AACA;AACA,iBAAiB,qCAAqC;AACtD,iBAAiB,oCAAoC;AACrD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,iBAAiB,sBAAsB;AACvC;AACA;AACA,sBAAsB,2BAA2B;AACjD;AACA;AACA;AACA,sBAAsB,6BAA6B;AACnD;AACA,sBAAsB,6BAA6B;AACnD;AACA,sBAAsB,6BAA6B;AACnD;AACA,sBAAsB,6BAA6B;AACnD;AACA,sBAAsB,6BAA6B;AACnD;AACA,sBAAsB,6BAA6B;AACnD;AACA,sBAAsB,6BAA6B;AACnD;AACA,sBAAsB,2BAA2B;AACjD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,iBAAiB,sBAAsB;AACvC;AACA;AACA,sBAAsB,2BAA2B;AACjD;AACA;AACA;AACA,sBAAsB,2BAA2B;AACjD;AACA;AACA;AACA,sBAAsB,6BAA6B;AACnD;AACA,sBAAsB,6BAA6B;AACnD;AACA,sBAAsB,6BAA6B;AACnD;AACA,sBAAsB,6BAA6B;AACnD;AACA,sBAAsB,6BAA6B;AACnD;AACA,sBAAsB,6BAA6B;AACnD;AACA,sBAAsB,6BAA6B;AACnD;AACA,sBAAsB,6BAA6B;AACnD;AACA,sBAAsB,2BAA2B;AACjD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,iBAAiB,sBAAsB;AACvC;AACA;AACA,sBAAsB,2BAA2B;AACjD;AACA;AACA;AACA,sBAAsB,6BAA6B;AACnD;AACA,sBAAsB,6BAA6B;AACnD;AACA,sBAAsB,6BAA6B;AACnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,iBAAiB,sBAAsB;AACvC;AACA;AACA,sBAAsB,2BAA2B;AACjD;AACA,sBAAsB,2BAA2B;AACjD;AACA,sBAAsB,6BAA6B;AACnD;AACA;AACA;AACA,sBAAsB,6BAA6B;AACnD;AACA,sBAAsB,8BAA8B;AACpD;AACA,sBAAsB,8BAA8B;AACpD;AACA,sBAAsB,2BAA2B;AACjD;AACA,sBAAsB,6BAA6B;AACnD;AACA,sBAAsB,6BAA6B;AACnD,sBAAsB,6BAA6B;AACnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,iBAAiB,sBAAsB;AACvC;AACA;AACA,sBAAsB,8BAA8B;AACpD;AACA;AACA;AACA,sBAAsB,2BAA2B;AACjD;AACA;AACA;AACA,sBAAsB,8BAA8B;AACpD;AACA;AACA;AACA,sBAAsB,2BAA2B;AACjD;AACA;AACA;AACA,sBAAsB,2BAA2B;AACjD;AACA,sBAAsB,8BAA8B;AACpD;AACA;AACA;AACA,sBAAsB,2BAA2B;AACjD;AACA;AACA;AACA,sBAAsB,2BAA2B;AACjD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,iBAAiB,sBAAsB;AACvC;AACA;AACA,sBAAsB,2BAA2B;AACjD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACxwBA;AAAA;AAAA;;AAEA;AACA;AACA;;AAEe;AACf;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA,yBAAyB;AACzB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA,qBAAqB;AACrB;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;AC5FA;AAAA;AAAA;AAA6G;AAC7G;;;AAGA;AAC6F;AAC7F,gBAAgB,2GAAU;AAC1B;AACA,EAAE,sGAAM;AACR,EAAE,+GAAe;AACjB;AACA;AACA;AACA;;AAEA;;AAEA;AACA,IAAI,KAAU,EAAE,YAiBf;AACD;AACe,gF;;;;;;;;;;;;ACrCf;AAAA;AAAA;AAAA;AAAA;AAAA","file":"/docs/hadoop/-1b265fff.js","sourcesContent":[" \t// The module cache\n \tvar installedModules = {};\n\n \t// The require function\n \tfunction __webpack_require__(moduleId) {\n\n \t\t// Check if module is in cache\n \t\tif(installedModules[moduleId]) {\n \t\t\treturn installedModules[moduleId].exports;\n \t\t}\n \t\t// Create a new module (and put it into the cache)\n \t\tvar module = installedModules[moduleId] = {\n \t\t\ti: moduleId,\n \t\t\tl: false,\n \t\t\texports: {}\n \t\t};\n\n \t\t// Execute the module function\n \t\tmodules[moduleId].call(module.exports, module, module.exports, __webpack_require__);\n\n \t\t// Flag the module as loaded\n \t\tmodule.l = true;\n\n \t\t// Return the exports of the module\n \t\treturn module.exports;\n \t}\n\n\n \t// expose the modules object (__webpack_modules__)\n \t__webpack_require__.m = modules;\n\n \t// expose the module cache\n \t__webpack_require__.c = installedModules;\n\n \t// define getter function for harmony exports\n \t__webpack_require__.d = function(exports, name, getter) {\n \t\tif(!__webpack_require__.o(exports, name)) {\n \t\t\tObject.defineProperty(exports, name, { enumerable: true, get: getter });\n \t\t}\n \t};\n\n \t// define __esModule on exports\n \t__webpack_require__.r = function(exports) {\n \t\tif(typeof Symbol !== 'undefined' && Symbol.toStringTag) {\n \t\t\tObject.defineProperty(exports, Symbol.toStringTag, { value: 'Module' });\n \t\t}\n \t\tObject.defineProperty(exports, '__esModule', { value: true });\n \t};\n\n \t// create a fake namespace object\n \t// mode & 1: value is a module id, require it\n \t// mode & 2: merge all properties of value into the ns\n \t// mode & 4: return value when already ns object\n \t// mode & 8|1: behave like require\n \t__webpack_require__.t = function(value, mode) {\n \t\tif(mode & 1) value = __webpack_require__(value);\n \t\tif(mode & 8) return value;\n \t\tif((mode & 4) && typeof value === 'object' && value && value.__esModule) return value;\n \t\tvar ns = Object.create(null);\n \t\t__webpack_require__.r(ns);\n \t\tObject.defineProperty(ns, 'default', { enumerable: true, value: value });\n \t\tif(mode & 2 && typeof value != 'string') for(var key in value) __webpack_require__.d(ns, key, function(key) { return value[key]; }.bind(null, key));\n \t\treturn ns;\n \t};\n\n \t// getDefaultExport function for compatibility with non-harmony modules\n \t__webpack_require__.n = function(module) {\n \t\tvar getter = module && module.__esModule ?\n \t\t\tfunction getDefault() { return module['default']; } :\n \t\t\tfunction getModuleExports() { return module; };\n \t\t__webpack_require__.d(getter, 'a', getter);\n \t\treturn getter;\n \t};\n\n \t// Object.prototype.hasOwnProperty.call\n \t__webpack_require__.o = function(object, property) { return Object.prototype.hasOwnProperty.call(object, property); };\n\n \t// __webpack_public_path__\n \t__webpack_require__.p = \"\";\n\n\n \t// Load entry module and return exports\n \treturn __webpack_require__(__webpack_require__.s = \"./src/pages/md/bigData_hadoop_基础_1-1_HdfsShell.md\");\n","var render = function() {\n  var _vm = this\n  var _h = _vm.$createElement\n  var _c = _vm._self._c || _h\n  return _vm._m(0)\n}\nvar staticRenderFns = [\n  function() {\n    var _vm = this\n    var _h = _vm.$createElement\n    var _c = _vm._self._c || _h\n    return _c(\"div\", { staticClass: \"content\" }, [\n      _c(\"h1\", { attrs: { id: \"hadoop-filesystem-shell\" } }, [\n        _c(\n          \"a\",\n          {\n            staticClass: \"header-anchor\",\n            attrs: { href: \"#hadoop-filesystem-shell\", \"aria-hidden\": \"true\" }\n          },\n          [_vm._v(\"#\")]\n        ),\n        _vm._v(\" Hadoop FileSystem Shell\")\n      ]),\n      _vm._v(\" \"),\n      _c(\"p\", [_vm._v(\"hadoop  命令全局变量设置\")]),\n      _vm._v(\" \"),\n      _c(\"pre\", { staticClass: \"hljs\" }, [\n        _c(\"code\", [\n          _vm._v(\n            \"   vi /etc/profile\\n   export JAVA_HOME=/opt/java/jdk1.8.0_181\\n\\t#export HADOOP_HOME=/opt/hadoop/hadoop-\"\n          ),\n          _c(\"span\", { staticClass: \"hljs-number\" }, [_vm._v(\"2.7\")]),\n          _vm._v(\n            \".1\\n\\texport HADOOP_HOME=/opt/hadoop/pseudo_hadoop\\n\\texport JRE_HOME=$JAVA_HOME/jre\\n\\texport CLASSPATH=.:$JAVA_HOME/lib:$JRE_HOME/lib:$CLASSPATH\\n\\texport PATH=$JAVA_HOME/bin:$JRE_HOME/bin:$HADOOP_HOME/sbin:$HADOOP_HOME/bin:$PATH\\n\\texport PATH=/opt/scala/scala-\"\n          ),\n          _c(\"span\", { staticClass: \"hljs-number\" }, [_vm._v(\"2.11\")]),\n          _vm._v(\n            \".12/bin:$PATH\\n\\t#config nexus oss3.x  ,repositity\\n\\texport NEXUS_HOME=/opt/nexus/nexus-\"\n          ),\n          _c(\"span\", { staticClass: \"hljs-number\" }, [_vm._v(\"3.13\")]),\n          _vm._v(\".0-\"),\n          _c(\"span\", { staticClass: \"hljs-number\" }, [_vm._v(\"01\")]),\n          _vm._v(\n            \"\\n\\t#config node v10\\n\\texport NODE_HOME=/opt/node/node-v10.13.0-linux-x64/bin\\n\\texport PATH=$NODE_HOME:$PATH\\n\\n\\n\"\n          )\n        ])\n      ]),\n      _vm._v(\" \"),\n      _c(\"div\", { staticClass: \"success custom-block\" }, [\n        _c(\"p\", { staticClass: \"custom-block-title\" }),\n        _vm._v(\" \"),\n        _c(\"pre\", { staticClass: \"hljs\" }, [\n          _c(\"code\", [\n            _vm._v(\"    [root\"),\n            _c(\"span\", { staticClass: \"hljs-meta\" }, [_vm._v(\"@hadoop\")]),\n            _vm._v(\"01 bin]# source /etc/profile\\n   \"),\n            _c(\"span\", { staticClass: \"hljs-comment\" }, [\n              _vm._v(\"// 配置成功\")\n            ]),\n            _vm._v(\"\\n    [root\"),\n            _c(\"span\", { staticClass: \"hljs-meta\" }, [_vm._v(\"@hadoop\")]),\n            _vm._v(\n              \"01 bin]# hadoop\\n   Usage: hadoop [--config confdir] [COMMAND | CLASSNAME]\\n     CLASSNAME            run the \"\n            ),\n            _c(\"span\", { staticClass: \"hljs-class\" }, [\n              _c(\"span\", { staticClass: \"hljs-keyword\" }, [_vm._v(\"class\")]),\n              _vm._v(\" \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"named\")]),\n              _vm._v(\" \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"CLASSNAME\")]),\n              _vm._v(\"\\n    \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"or\")]),\n              _vm._v(\"\\n     \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"where\")]),\n              _vm._v(\" \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"COMMAND\")]),\n              _vm._v(\" \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"is\")]),\n              _vm._v(\" \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"one\")]),\n              _vm._v(\" \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"of\")]),\n              _vm._v(\":\\n     \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"fs\")]),\n              _vm._v(\"                   \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"run\")]),\n              _vm._v(\" \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"a\")]),\n              _vm._v(\" \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"generic\")]),\n              _vm._v(\" \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"filesystem\")]),\n              _vm._v(\" \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"user\")]),\n              _vm._v(\" \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"client\")]),\n              _vm._v(\"\\n     \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"version\")]),\n              _vm._v(\"              \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"print\")]),\n              _vm._v(\" \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"the\")]),\n              _vm._v(\" \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"version\")]),\n              _vm._v(\"\\n     \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"jar\")]),\n              _vm._v(\" [\"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"jar\")]),\n              _vm._v(\"]            \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"run\")]),\n              _vm._v(\" \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"a\")]),\n              _vm._v(\" \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"jar\")]),\n              _vm._v(\" \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"file\")]),\n              _vm._v(\"\\n                          \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"note\")]),\n              _vm._v(\": \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"please\")]),\n              _vm._v(\" \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"use\")]),\n              _vm._v(' \"'),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"yarn\")]),\n              _vm._v(\" \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"jar\")]),\n              _vm._v('\" '),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"to\")]),\n              _vm._v(\" \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"launch\")]),\n              _vm._v(\"\\n                                \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"YARN\")]),\n              _vm._v(\" \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [\n                _vm._v(\"applications\")\n              ]),\n              _vm._v(\", \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"not\")]),\n              _vm._v(\" \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"this\")]),\n              _vm._v(\" \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"command\")]),\n              _vm._v(\".\\n     \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [\n                _vm._v(\"checknative\")\n              ]),\n              _vm._v(\" [-\"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"a\")]),\n              _vm._v(\"|-\"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"h\")]),\n              _vm._v(\"]  \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"check\")]),\n              _vm._v(\" \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"native\")]),\n              _vm._v(\" \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"hadoop\")]),\n              _vm._v(\" \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"and\")]),\n              _vm._v(\" \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [\n                _vm._v(\"compression\")\n              ]),\n              _vm._v(\" \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"libraries\")]),\n              _vm._v(\" \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [\n                _vm._v(\"availability\")\n              ]),\n              _vm._v(\"\\n     \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"distcp\")]),\n              _vm._v(\" [\"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"srcurl\")]),\n              _vm._v(\"] [\"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"desturl\")]),\n              _vm._v(\"] \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"copy\")]),\n              _vm._v(\" \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"file\")]),\n              _vm._v(\" \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"or\")]),\n              _vm._v(\" \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [\n                _vm._v(\"directories\")\n              ]),\n              _vm._v(\" \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [\n                _vm._v(\"recursively\")\n              ]),\n              _vm._v(\"\\n     \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"archive\")]),\n              _vm._v(\" -\"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [\n                _vm._v(\"archiveName\")\n              ]),\n              _vm._v(\" \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"NAME\")]),\n              _vm._v(\" -\"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"p\")]),\n              _vm._v(\" [\"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"parent\")]),\n              _vm._v(\" \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"path\")]),\n              _vm._v(\"] [\"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"src\")]),\n              _vm._v(\"]* [\"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"dest\")]),\n              _vm._v(\"] \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"create\")]),\n              _vm._v(\" \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"a\")]),\n              _vm._v(\" \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"hadoop\")]),\n              _vm._v(\" \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"archive\")]),\n              _vm._v(\"\\n     \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"classpath\")]),\n              _vm._v(\"            \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"prints\")]),\n              _vm._v(\" \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"the\")]),\n              _vm._v(\" \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"class\")]),\n              _vm._v(\" \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"path\")]),\n              _vm._v(\" \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"needed\")]),\n              _vm._v(\" \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"to\")]),\n              _vm._v(\" \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"get\")]),\n              _vm._v(\" \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"the\")]),\n              _vm._v(\"\\n     \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"credential\")]),\n              _vm._v(\"           \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"interact\")]),\n              _vm._v(\" \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"with\")]),\n              _vm._v(\" \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"credential\")]),\n              _vm._v(\" \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"providers\")]),\n              _vm._v(\"\\n                          \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"Hadoop\")]),\n              _vm._v(\" \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"jar\")]),\n              _vm._v(\" \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"and\")]),\n              _vm._v(\" \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"the\")]),\n              _vm._v(\" \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"required\")]),\n              _vm._v(\" \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"libraries\")]),\n              _vm._v(\"\\n     \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"daemonlog\")]),\n              _vm._v(\"            \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"get\")]),\n              _vm._v(\"/\"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"set\")]),\n              _vm._v(\" \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"the\")]),\n              _vm._v(\" \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"log\")]),\n              _vm._v(\" \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"level\")]),\n              _vm._v(\" \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"for\")]),\n              _vm._v(\" \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"each\")]),\n              _vm._v(\" \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"daemon\")]),\n              _vm._v(\"\\n     \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"trace\")]),\n              _vm._v(\"                \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"view\")]),\n              _vm._v(\" \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"and\")]),\n              _vm._v(\" \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"modify\")]),\n              _vm._v(\" \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"Hadoop\")]),\n              _vm._v(\" \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"tracing\")]),\n              _vm._v(\" \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"settings\")]),\n              _vm._v(\"\\n   \\n   \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"Most\")]),\n              _vm._v(\" \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"commands\")]),\n              _vm._v(\" \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"print\")]),\n              _vm._v(\" \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"help\")]),\n              _vm._v(\" \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"when\")]),\n              _vm._v(\" \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"invoked\")]),\n              _vm._v(\" \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"w\")]),\n              _vm._v(\"/\"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"o\")]),\n              _vm._v(\" \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"parameters\")]),\n              _vm._v(\".\\n   [\"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"root\")]),\n              _vm._v(\"@\"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"hadoop01\")]),\n              _vm._v(\" \"),\n              _c(\"span\", { staticClass: \"hljs-title\" }, [_vm._v(\"bin\")]),\n              _vm._v(\"]# \\n\")\n            ])\n          ])\n        ])\n      ]),\n      _vm._v(\" \"),\n      _c(\"p\", [_vm._v(\"#行动\")]),\n      _vm._v(\" \"),\n      _c(\"p\", [\n        _vm._v(\"首先我们创建一个hadoop的文件夹用于存放我们的练习shell command\")\n      ]),\n      _vm._v(\" \"),\n      _c(\"pre\", { staticClass: \"hljs\" }, [\n        _c(\"code\", [\n          _vm._v(\"[root\"),\n          _c(\"span\", { staticClass: \"hljs-meta\" }, [_vm._v(\"@hadoop\")]),\n          _vm._v(\"01 hadoopfileSystem]# hadoop fs -ls /user\\n[root\"),\n          _c(\"span\", { staticClass: \"hljs-meta\" }, [_vm._v(\"@hadoop\")]),\n          _vm._v(\n            \"01 hadoopfileSystem]# hadoop fs -mkdir /user/fileShellDemo \\n\\n\"\n          )\n        ])\n      ]),\n      _vm._v(\" \"),\n      _c(\"ul\", [_c(\"li\", [_vm._v(\"文件操作命令\")])]),\n      _vm._v(\" \"),\n      _c(\"p\", [_vm._v(\"cmd: appendToFile\")]),\n      _vm._v(\" \"),\n      _c(\"pre\", [\n        _c(\"code\", [\n          _vm._v(\n            \"Usage: hadoop fs -appendToFile [localsrc] ... [dst]\\n\\nAppend single src, or multiple srcs from local file system to the destination file system. Also reads input from stdin and appends to destination file system.\\n\\nhadoop fs -appendToFile localfile /user/hadoop/hadoopfile\\nhadoop fs -appendToFile localfile1 localfile2 /user/hadoop/hadoopfile\\nhadoop fs -appendToFile localfile hdfs://nn.example.com/hadoop/hadoopfile\\nhadoop fs -appendToFile - hdfs://nn.example.com/hadoop/hadoopfile Reads the input from stdin.\\nExit Code:\\n\\nReturns 0 on success and 1 on error.\\n\\n:::warnning\\n\\n    [root@hadoop01 hadoopfileSystem]# hadoop fs -appendToFile hello.txt /user/fileShellDemo\\n17/06/29 20:08:33 WARN hdfs.DFSClient: DataStreamer Exception\\norg.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/fileShellDemo could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.\\n\\n:::\\n\"\n          )\n        ])\n      ]),\n      _vm._v(\" \"),\n      _c(\"p\", [_vm._v(\"居然报错了，说的是nodeData 节点没有运行\")]),\n      _vm._v(\" \"),\n      _c(\"p\", [_vm._v(\"查看日志：hadoop中datanode的log文件\")]),\n      _vm._v(\" \"),\n      _c(\"pre\", [\n        _c(\"code\", [\n          _vm._v(\n            \"cd  logs\\n\\ncat hadoop-root-datanode-hadoop01.log\\n\\n```\\n\\t2017-06-29 20:27:02,806 WARN org.apache.hadoop.hdfs.server.common.Storage: java.io.IOException: Incompatible clusterIDs in /usr/hadoop_pseudo/tmp/datanode: namenode clusterID = CID-02320cff-7374-444d-9992-a405377cbdb0; datanode clusterID = CID-4310c6eb-ad78-4ca6-971a-ff20017f6b25\\n\\t2019-06-29 20:27:02,807 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for Block pool [registering] (Datanode Uuid unassigned) service to hadoop01/192.168.1.125:9000. Exiting. \\n```\\n\\n发现是由于name 和 data 节点的 ClusterID 不同导致。只需要保持相同即可。 \\n\\n[root@hadoop01 current]# pwd\\n/usr/hadoop_pseudo/tmp/namenode/current\\n[root@hadoop01 current]# vi VERSION \\n\\n即把 datanode 文件下的clusterID 拷贝到  namenode 即可。\\n\\n重新启动服务\\n\"\n          )\n        ])\n      ]),\n      _vm._v(\" \"),\n      _c(\"pre\", { staticClass: \"hljs\" }, [\n        _c(\"code\", [\n          _vm._v(\"\\n\\t[root\"),\n          _c(\"span\", { staticClass: \"hljs-meta\" }, [_vm._v(\"@hadoop\")]),\n          _vm._v(\"01 sbin]# jps\\n\\t\"),\n          _c(\"span\", { staticClass: \"hljs-number\" }, [_vm._v(\"29957\")]),\n          _vm._v(\" NameNode\\n\\t\"),\n          _c(\"span\", { staticClass: \"hljs-number\" }, [_vm._v(\"30565\")]),\n          _vm._v(\" NodeManager\\n\\t\"),\n          _c(\"span\", { staticClass: \"hljs-number\" }, [_vm._v(\"30262\")]),\n          _vm._v(\" SecondaryNameNode\\n\\t\"),\n          _c(\"span\", { staticClass: \"hljs-number\" }, [_vm._v(\"30457\")]),\n          _vm._v(\" ResourceManager\\n\\t\"),\n          _c(\"span\", { staticClass: \"hljs-number\" }, [_vm._v(\"30650\")]),\n          _vm._v(\" Jps\\n\\t\"),\n          _c(\"span\", { staticClass: \"hljs-number\" }, [_vm._v(\"30059\")]),\n          _vm._v(\" DataNode\\n\\t\"),\n          _c(\"span\", { staticClass: \"hljs-number\" }, [_vm._v(\"5038\")]),\n          _vm._v(\" UnixLauncher\\n\\t[root\"),\n          _c(\"span\", { staticClass: \"hljs-meta\" }, [_vm._v(\"@hadoop\")]),\n          _vm._v(\"01 sbin]# \\n\\n\")\n        ])\n      ]),\n      _vm._v(\" \"),\n      _c(\"div\", { staticClass: \"danger custom-block\" }, [\n        _c(\"p\", { staticClass: \"custom-block-title\" }),\n        _vm._v(\" \"),\n        _c(\"pre\", [\n          _c(\"code\", [\n            _vm._v(\n              \" 多次格式化namenode节点导致的，所以格式节点只有在第一次启动hadoop的时候。\\n\"\n            )\n          ])\n        ])\n      ]),\n      _vm._v(\" \"),\n      _c(\"pre\", { staticClass: \"hljs\" }, [\n        _c(\"code\", [\n          _vm._v(\"\\t[root\"),\n          _c(\"span\", { staticClass: \"hljs-meta\" }, [_vm._v(\"@hadoop\")]),\n          _vm._v(\n            \"01 hadoopfileSystem]# hadoop fs -ls /user/fileShellDemo\\n\\t-rw-r--r--   \"\n          ),\n          _c(\"span\", { staticClass: \"hljs-number\" }, [_vm._v(\"1\")]),\n          _vm._v(\" root supergroup          \"),\n          _c(\"span\", { staticClass: \"hljs-number\" }, [_vm._v(\"5\")]),\n          _vm._v(\" \"),\n          _c(\"span\", { staticClass: \"hljs-number\" }, [_vm._v(\"2017\")]),\n          _vm._v(\"-\"),\n          _c(\"span\", { staticClass: \"hljs-number\" }, [_vm._v(\"06\")]),\n          _vm._v(\"-\"),\n          _c(\"span\", { staticClass: \"hljs-number\" }, [_vm._v(\"29\")]),\n          _vm._v(\" \"),\n          _c(\"span\", { staticClass: \"hljs-number\" }, [_vm._v(\"21\")]),\n          _vm._v(\":\"),\n          _c(\"span\", { staticClass: \"hljs-number\" }, [_vm._v(\"00\")]),\n          _vm._v(\" /user/fileShellDemo\\n\\t[root\"),\n          _c(\"span\", { staticClass: \"hljs-meta\" }, [_vm._v(\"@hadoop\")]),\n          _vm._v(\"01 hadoopfileSystem]# \\n\\t\\n\")\n        ])\n      ]),\n      _vm._v(\" \"),\n      _c(\"ul\", [\n        _c(\"li\", [\n          _c(\"p\", [_vm._v(\"copyFromLocal\")]),\n          _vm._v(\" \"),\n          _c(\"p\", [_vm._v(\"Usage: hadoop fs -copyFromLocal [localsrc] URI\")]),\n          _vm._v(\" \"),\n          _c(\"p\", [\n            _vm._v(\n              \"Similar to the fs -put command, except that the source is restricted to a local file reference.\"\n            )\n          ]),\n          _vm._v(\" \"),\n          _c(\"p\", [_vm._v(\"Options:\")]),\n          _vm._v(\" \"),\n          _c(\"p\", [\n            _vm._v(\n              \"-p : Preserves access and modification times, ownership and the permissions. (assuming the permissions can be propagated across filesystems)\\n-f : Overwrites the destination if it already exists.\\n-l : Allow DataNode to lazily persist the file to disk, Forces a replication factor of 1. This flag will result in reduced durability. Use with care.\\n-d : Skip creation of temporary file with the suffix .\"\n            ),\n            _c(\"em\", [_vm._v(\"COPYING\")]),\n            _vm._v(\".\")\n          ])\n        ])\n      ]),\n      _vm._v(\" \"),\n      _c(\"pre\", { staticClass: \"hljs\" }, [\n        _c(\"code\", [\n          _vm._v(\"\\t [root\"),\n          _c(\"span\", { staticClass: \"hljs-meta\" }, [_vm._v(\"@hadoop\")]),\n          _vm._v(\n            \"01 hadoopfileSystem]# hadoop fs -copyFromLocal hello2.txt  /user/fileShellDemo\\n\\t[root\"\n          ),\n          _c(\"span\", { staticClass: \"hljs-meta\" }, [_vm._v(\"@hadoop\")]),\n          _vm._v(\n            \"01 hadoopfileSystem]# hadoop fs -ls /user/fileShellDemo\\n\\t\\tFound \"\n          ),\n          _c(\"span\", { staticClass: \"hljs-number\" }, [_vm._v(\"1\")]),\n          _vm._v(\" items\\n\\t\\t-rw-r--r--   \"),\n          _c(\"span\", { staticClass: \"hljs-number\" }, [_vm._v(\"1\")]),\n          _vm._v(\" root supergroup          \"),\n          _c(\"span\", { staticClass: \"hljs-number\" }, [_vm._v(\"7\")]),\n          _vm._v(\" \"),\n          _c(\"span\", { staticClass: \"hljs-number\" }, [_vm._v(\"2017\")]),\n          _vm._v(\"-\"),\n          _c(\"span\", { staticClass: \"hljs-number\" }, [_vm._v(\"06\")]),\n          _vm._v(\"-\"),\n          _c(\"span\", { staticClass: \"hljs-number\" }, [_vm._v(\"29\")]),\n          _vm._v(\" \"),\n          _c(\"span\", { staticClass: \"hljs-number\" }, [_vm._v(\"21\")]),\n          _vm._v(\":\"),\n          _c(\"span\", { staticClass: \"hljs-number\" }, [_vm._v(\"13\")]),\n          _vm._v(\" /user/fileShellDemo/hello2.txt\\n\\t[root\"),\n          _c(\"span\", { staticClass: \"hljs-meta\" }, [_vm._v(\"@hadoop\")]),\n          _vm._v(\"01 hadoopfileSystem]# \\n\\n\")\n        ])\n      ]),\n      _vm._v(\" \"),\n      _c(\"ul\", [\n        _c(\"li\", [\n          _c(\"p\", [_vm._v(\"copyToLocal\")]),\n          _vm._v(\" \"),\n          _c(\"p\", [\n            _vm._v(\n              \"Usage: hadoop fs -copyToLocal [-ignorecrc] [-crc] URI [localdst]\"\n            )\n          ]),\n          _vm._v(\" \"),\n          _c(\"p\", [\n            _vm._v(\n              \"Similar to get command, except that the destination is restricted to a local file reference.\"\n            )\n          ])\n        ]),\n        _vm._v(\" \"),\n        _c(\"li\", [\n          _c(\"p\", [_vm._v(\"count\")]),\n          _vm._v(\" \"),\n          _c(\"p\", [\n            _vm._v(\n              \"Usage: hadoop fs -count [-q] [-h] [-v] [-x] [-t [[storage type]]] [-u] [paths]\"\n            )\n          ]),\n          _vm._v(\" \"),\n          _c(\"p\", [\n            _vm._v(\n              \"Count the number of directories, files and bytes under the paths that match the specified file pattern. Get the quota and the usage. The output columns with -count are: DIR_COUNT, FILE_COUNT, CONTENT_SIZE, PATHNAME\"\n            )\n          ]),\n          _vm._v(\" \"),\n          _c(\"p\", [\n            _vm._v(\n              \"The -u and -q options control what columns the output contains. -q means show quotas, -u limits the output to show quotas and usage only.\"\n            )\n          ]),\n          _vm._v(\" \"),\n          _c(\"p\", [\n            _vm._v(\n              \"The output columns with -count -q are: QUOTA, REMAINING_QUOTA, SPACE_QUOTA, REMAINING_SPACE_QUOTA, DIR_COUNT, FILE_COUNT, CONTENT_SIZE, PATHNAME\"\n            )\n          ]),\n          _vm._v(\" \"),\n          _c(\"p\", [\n            _vm._v(\n              \"The output columns with -count -u are: QUOTA, REMAINING_QUOTA, SPACE_QUOTA, REMAINING_SPACE_QUOTA, PATHNAME\"\n            )\n          ]),\n          _vm._v(\" \"),\n          _c(\"p\", [\n            _vm._v(\n              'The -t option shows the quota and usage for each storage type. The -t option is ignored if -u or -q option is not given. The list of possible parameters that can be used in -t option(case insensitive except the parameter \"“): ”“, ”all“, ”ram_disk“, ”ssd“, ”disk“ or ”archive\".'\n            )\n          ]),\n          _vm._v(\" \"),\n          _c(\"p\", [\n            _vm._v(\"The -h option shows sizes in human readable format.\")\n          ]),\n          _vm._v(\" \"),\n          _c(\"p\", [_vm._v(\"The -v option displays a header line.\")]),\n          _vm._v(\" \"),\n          _c(\"p\", [\n            _vm._v(\n              \"The -x option excludes snapshots from the result calculation. Without the -x option (default), the result is always calculated from all INodes, including all snapshots under the given path. The -x option is ignored if -u or -q option is given.\"\n            )\n          ]),\n          _vm._v(\" \"),\n          _c(\"p\", [_vm._v(\"Example:\")]),\n          _vm._v(\" \"),\n          _c(\"p\", [\n            _vm._v(\n              \"hadoop fs -count hdfs://nn1.example.com/file1 hdfs://nn2.example.com/file2\\nhadoop fs -count -q hdfs://nn1.example.com/file1\\nhadoop fs -count -q -h hdfs://nn1.example.com/file1\\nhadoop fs -count -q -h -v hdfs://nn1.example.com/file1\\nhadoop fs -count -u hdfs://nn1.example.com/file1\\nhadoop fs -count -u -h hdfs://nn1.example.com/file1\\nhadoop fs -count -u -h -v hdfs://nn1.example.com/file1\\nExit Code:\"\n            )\n          ]),\n          _vm._v(\" \"),\n          _c(\"p\", [_vm._v(\"Returns 0 on success and -1 on error.\")])\n        ])\n      ]),\n      _vm._v(\" \"),\n      _c(\"pre\", { staticClass: \"hljs\" }, [\n        _c(\"code\", [\n          _vm._v(\"[root\"),\n          _c(\"span\", { staticClass: \"hljs-meta\" }, [_vm._v(\"@hadoop\")]),\n          _vm._v(\n            \"01 hadoopfileSystem]# hadoop fs -count -h /user/fileShellDemo/hello2.txt\\n           \"\n          ),\n          _c(\"span\", { staticClass: \"hljs-number\" }, [_vm._v(\"0\")]),\n          _vm._v(\"            \"),\n          _c(\"span\", { staticClass: \"hljs-number\" }, [_vm._v(\"1\")]),\n          _vm._v(\"                  \"),\n          _c(\"span\", { staticClass: \"hljs-number\" }, [_vm._v(\"7\")]),\n          _vm._v(\" /user/fileShellDemo/hello2.txt\\n\")\n        ])\n      ]),\n      _vm._v(\" \"),\n      _c(\"ul\", [\n        _c(\"li\", [\n          _c(\"p\", [_vm._v(\"truncate\")]),\n          _vm._v(\" \"),\n          _c(\"p\", [_vm._v(\"Usage: hadoop fs -truncate [-w] [length] [paths]\")]),\n          _vm._v(\" \"),\n          _c(\"p\", [\n            _vm._v(\n              \"Truncate all files that match the specified file pattern to the specified length.\"\n            )\n          ]),\n          _vm._v(\" \"),\n          _c(\"p\", [_vm._v(\"Options:\")]),\n          _vm._v(\" \"),\n          _c(\"p\", [\n            _vm._v(\n              \"The -w flag requests that the command waits for block recovery to complete, if necessary. Without -w flag the file may remain unclosed for some time while the recovery is in progress. During this time file cannot be reopened for append.\\nExample:\"\n            )\n          ]),\n          _vm._v(\" \"),\n          _c(\"p\", [\n            _vm._v(\n              \"hadoop fs -truncate 55 /user/hadoop/file1 /user/hadoop/file2\\nhadoop fs -truncate -w 127 hdfs://nn1.example.com/user/hadoop/file1\"\n            )\n          ])\n        ])\n      ]),\n      _vm._v(\" \"),\n      _c(\"pre\", { staticClass: \"hljs\" }, [\n        _c(\"code\", [\n          _vm._v(\"\\n   [root\"),\n          _c(\"span\", { staticClass: \"hljs-meta\" }, [_vm._v(\"@hadoop\")]),\n          _vm._v(\"01 hadoopfileSystem]# cat hello2.txt \\n\\thello2\\n\\t[root\"),\n          _c(\"span\", { staticClass: \"hljs-meta\" }, [_vm._v(\"@hadoop\")]),\n          _vm._v(\"01 hadoopfileSystem]# hadoop fs -truncate \"),\n          _c(\"span\", { staticClass: \"hljs-number\" }, [_vm._v(\"2\")]),\n          _vm._v(\n            \" /user/fileShellDemo/hello2.txt\\n\\tTruncating /user/fileShellDemo/hello2.txt to length: \"\n          ),\n          _c(\"span\", { staticClass: \"hljs-number\" }, [_vm._v(\"2\")]),\n          _vm._v(\". Wait \"),\n          _c(\"span\", { staticClass: \"hljs-keyword\" }, [_vm._v(\"for\")]),\n          _vm._v(\" block recovery to complete before further updating \"),\n          _c(\"span\", { staticClass: \"hljs-keyword\" }, [_vm._v(\"this\")]),\n          _vm._v(\" file.\\n\\t[root\"),\n          _c(\"span\", { staticClass: \"hljs-meta\" }, [_vm._v(\"@hadoop\")]),\n          _vm._v(\"01 hadoopfileSystem]# hadoop fs -cat \"),\n          _c(\"span\", { staticClass: \"hljs-number\" }, [_vm._v(\"2\")]),\n          _vm._v(\" /user/fileShellDemo/hello2.txt\\n\\tcat: `\"),\n          _c(\"span\", { staticClass: \"hljs-number\" }, [_vm._v(\"2\")]),\n          _c(\"span\", { staticClass: \"hljs-string\" }, [\n            _vm._v(\n              \"': No such file or directory\\n\\the[root@hadoop01 hadoopfileSystem]# hadoop fs -cat /user/fileShellDemo/hello2.txt\\n\\the[root@hadoop01 hadoopfileSystem]# \\n\\n\"\n            )\n          ])\n        ])\n      ]),\n      _vm._v(\" \"),\n      _c(\"ul\", [\n        _c(\"li\", [\n          _c(\"p\", [_vm._v(\"find\")]),\n          _vm._v(\" \"),\n          _c(\"p\", [\n            _vm._v(\"Usage: hadoop fs -find  [path] ... [expression] ...\")\n          ]),\n          _vm._v(\" \"),\n          _c(\"p\", [\n            _vm._v(\n              \"Finds all files that match the specified expression and applies selected actions to them. If no path is specified then defaults to the current working directory. If no expression is specified then defaults to -print.\"\n            )\n          ]),\n          _vm._v(\" \"),\n          _c(\"p\", [\n            _vm._v(\"The following primary expressions are recognised:\")\n          ]),\n          _vm._v(\" \"),\n          _c(\"p\", [_vm._v(\"-name pattern\\n-iname pattern\")]),\n          _vm._v(\" \"),\n          _c(\"p\", [\n            _vm._v(\n              \"Evaluates as true if the basename of the file matches the pattern using standard file system globbing. If -iname is used then the match is case insensitive.\"\n            )\n          ]),\n          _vm._v(\" \"),\n          _c(\"p\", [_vm._v(\"-print\\n-print0\")]),\n          _vm._v(\" \"),\n          _c(\"p\", [\n            _vm._v(\n              \"Always evaluates to true. Causes the current pathname to be written to standard output. If the -print0 expression is used then an ASCII NULL character is appended.\"\n            )\n          ]),\n          _vm._v(\" \"),\n          _c(\"p\", [_vm._v(\"The following operators are recognised:\")]),\n          _vm._v(\" \"),\n          _c(\"p\", [\n            _vm._v(\n              \"expression -a expression\\nexpression -and expression\\nexpression expression\"\n            )\n          ]),\n          _vm._v(\" \"),\n          _c(\"p\", [\n            _vm._v(\n              \"Logical AND operator for joining two expressions. Returns true if both child expressions return true. Implied by the juxtaposition of two expressions and so does not need to be explicitly specified. The second expression will not be applied if the first fails.\"\n            )\n          ]),\n          _vm._v(\" \"),\n          _c(\"p\", [_vm._v(\"Example:\")]),\n          _vm._v(\" \"),\n          _c(\"p\", [_vm._v(\"hadoop fs -find / -name test -print\")]),\n          _vm._v(\" \"),\n          _c(\"p\", [_vm._v(\"Exit Code:\")]),\n          _vm._v(\" \"),\n          _c(\"p\", [_vm._v(\"Returns 0 on success and -1 on error.\")])\n        ])\n      ]),\n      _vm._v(\" \"),\n      _c(\"pre\", { staticClass: \"hljs\" }, [\n        _c(\"code\", [\n          _vm._v(\"   \"),\n          _c(\"span\", { staticClass: \"hljs-comment\" }, [\n            _vm._v(\"//指定路径查找\")\n          ]),\n          _vm._v(\"\\n   [root\"),\n          _c(\"span\", { staticClass: \"hljs-meta\" }, [_vm._v(\"@hadoop\")]),\n          _vm._v(\n            \"01 hadoopfileSystem]# hadoop fs -find /user\\n\\t/user\\n\\t/user/fileShellDemo\\n\\t/user/fileShellDemo/hello2.txt\\n\\t\\n\\t\"\n          ),\n          _c(\"span\", { staticClass: \"hljs-comment\" }, [\n            _vm._v(\"//使用名字查找\")\n          ]),\n          _vm._v(\"\\n\\t\\n\\t[root\"),\n          _c(\"span\", { staticClass: \"hljs-meta\" }, [_vm._v(\"@hadoop\")]),\n          _vm._v(\n            \"01 hadoopfileSystem]# hadoop fs -find /user -name hello2.txt\\n\\t/user/fileShellDemo/hello2.txt\\n\\t[root\"\n          ),\n          _c(\"span\", { staticClass: \"hljs-meta\" }, [_vm._v(\"@hadoop\")]),\n          _vm._v(\"01 hadoopfileSystem]# \\n\\n    \"),\n          _c(\"span\", { staticClass: \"hljs-comment\" }, [\n            _vm._v(\"//不区分大小写查找\")\n          ]),\n          _vm._v(\"\\n    \\n    [root\"),\n          _c(\"span\", { staticClass: \"hljs-meta\" }, [_vm._v(\"@hadoop\")]),\n          _vm._v(\n            \"01 hadoopfileSystem]# hadoop fs -find /user  -iname HELLO2.txt\\n\\t/user/fileShellDemo/hello2.txt\\n\\t[root\"\n          ),\n          _c(\"span\", { staticClass: \"hljs-meta\" }, [_vm._v(\"@hadoop\")]),\n          _vm._v(\"01 hadoopfileSystem]# \\n\\t\\n\")\n        ])\n      ]),\n      _vm._v(\" \"),\n      _c(\"ul\", [\n        _c(\"li\", [\n          _c(\"p\", [_vm._v(\"test\")]),\n          _vm._v(\" \"),\n          _c(\"p\", [_vm._v(\"Usage: hadoop fs -test -[defsz] URI\")]),\n          _vm._v(\" \"),\n          _c(\"p\", [_vm._v(\"Options:\")]),\n          _vm._v(\" \"),\n          _c(\"p\", [\n            _vm._v(\n              \"-d: f the path is a directory, return 0.\\n-e: if the path exists, return 0.\\n-f: if the path is a file, return 0.\\n-s: if the path is not empty, return 0.\\n-r: if the path exists and read permission is granted, return 0.\\n-w: if the path exists and write permission is granted, return 0.\\n-z: if the file is zero length, return 0.\\nExample:\"\n            )\n          ])\n        ])\n      ]),\n      _vm._v(\" \"),\n      _c(\"p\", [_vm._v(\"hadoop fs -test -e filename\")]),\n      _vm._v(\" \"),\n      _c(\"ul\", [\n        _c(\"li\", [\n          _c(\"p\", [_vm._v(\"tail\")]),\n          _vm._v(\" \"),\n          _c(\"p\", [_vm._v(\"Usage: hadoop fs -tail [-f] URI\")]),\n          _vm._v(\" \"),\n          _c(\"p\", [_vm._v(\"Displays last kilobyte of the file to stdout.\")]),\n          _vm._v(\" \"),\n          _c(\"p\", [_vm._v(\"Options:\")]),\n          _vm._v(\" \"),\n          _c(\"p\", [\n            _vm._v(\n              \"The -f option will output appended data as the file grows, as in Unix.\\nExample:\"\n            )\n          ]),\n          _vm._v(\" \"),\n          _c(\"p\", [\n            _vm._v(\n              \"hadoop fs -tail pathname\\nExit Code: Returns 0 on success and -1 on error.\"\n            )\n          ])\n        ])\n      ]),\n      _vm._v(\" \"),\n      _c(\"pre\", { staticClass: \"hljs\" }, [\n        _c(\"code\", [\n          _vm._v(\"[root\"),\n          _c(\"span\", { staticClass: \"hljs-meta\" }, [_vm._v(\"@hadoop\")]),\n          _vm._v(\n            \"01 hadoopfileSystem]# hadoop fs -tail /user/fileShellDemo/hello2.txt\\nhe\\n\"\n          )\n        ])\n      ])\n    ])\n  }\n]\nrender._withStripped = true\n\nexport { render, staticRenderFns }","/* globals __VUE_SSR_CONTEXT__ */\n\n// IMPORTANT: Do NOT use ES2015 features in this file (except for modules).\n// This module is a runtime utility for cleaner component module output and will\n// be included in the final webpack user bundle.\n\nexport default function normalizeComponent (\n  scriptExports,\n  render,\n  staticRenderFns,\n  functionalTemplate,\n  injectStyles,\n  scopeId,\n  moduleIdentifier, /* server only */\n  shadowMode /* vue-cli only */\n) {\n  // Vue.extend constructor export interop\n  var options = typeof scriptExports === 'function'\n    ? scriptExports.options\n    : scriptExports\n\n  // render functions\n  if (render) {\n    options.render = render\n    options.staticRenderFns = staticRenderFns\n    options._compiled = true\n  }\n\n  // functional template\n  if (functionalTemplate) {\n    options.functional = true\n  }\n\n  // scopedId\n  if (scopeId) {\n    options._scopeId = 'data-v-' + scopeId\n  }\n\n  var hook\n  if (moduleIdentifier) { // server build\n    hook = function (context) {\n      // 2.3 injection\n      context =\n        context || // cached call\n        (this.$vnode && this.$vnode.ssrContext) || // stateful\n        (this.parent && this.parent.$vnode && this.parent.$vnode.ssrContext) // functional\n      // 2.2 with runInNewContext: true\n      if (!context && typeof __VUE_SSR_CONTEXT__ !== 'undefined') {\n        context = __VUE_SSR_CONTEXT__\n      }\n      // inject component styles\n      if (injectStyles) {\n        injectStyles.call(this, context)\n      }\n      // register component module identifier for async chunk inferrence\n      if (context && context._registeredComponents) {\n        context._registeredComponents.add(moduleIdentifier)\n      }\n    }\n    // used by ssr in case component is cached and beforeCreate\n    // never gets called\n    options._ssrRegister = hook\n  } else if (injectStyles) {\n    hook = shadowMode\n      ? function () { injectStyles.call(this, this.$root.$options.shadowRoot) }\n      : injectStyles\n  }\n\n  if (hook) {\n    if (options.functional) {\n      // for template-only hot-reload because in that case the render fn doesn't\n      // go through the normalizer\n      options._injectStyles = hook\n      // register for functioal component in vue file\n      var originalRender = options.render\n      options.render = function renderWithStyleInjection (h, context) {\n        hook.call(context)\n        return originalRender(h, context)\n      }\n    } else {\n      // inject component registration as beforeCreate hook\n      var existing = options.beforeCreate\n      options.beforeCreate = existing\n        ? [].concat(existing, hook)\n        : [hook]\n    }\n  }\n\n  return {\n    exports: scriptExports,\n    options: options\n  }\n}\n","import { render, staticRenderFns } from \"./bigData_hadoop_基础_1-1_HdfsShell.md?vue&type=template&id=6323f2a8&\"\nvar script = {}\n\n\n/* normalize component */\nimport normalizer from \"!../../../node_modules/vue-loader/lib/runtime/componentNormalizer.js\"\nvar component = normalizer(\n  script,\n  render,\n  staticRenderFns,\n  false,\n  null,\n  null,\n  null\n  \n)\n\n/* hot reload */\nif (module.hot) {\n  var api = require(\"/Users/fandong/Desktop/gitblogNew/node_modules/vue-hot-reload-api/dist/index.js\")\n  api.install(require('vue'))\n  if (api.compatible) {\n    module.hot.accept()\n    if (!api.isRecorded('6323f2a8')) {\n      api.createRecord('6323f2a8', component.options)\n    } else {\n      api.reload('6323f2a8', component.options)\n    }\n    module.hot.accept(\"./bigData_hadoop_基础_1-1_HdfsShell.md?vue&type=template&id=6323f2a8&\", function () {\n      api.rerender('6323f2a8', {\n        render: render,\n        staticRenderFns: staticRenderFns\n      })\n    })\n  }\n}\ncomponent.options.__file = \"src/pages/md/bigData_hadoop_基础_1-1_HdfsShell.md\"\nexport default component.exports","export * from \"-!../../../node_modules/vue-loader/lib/loaders/templateLoader.js??vue-loader-options!../../../node_modules/vue-loader/lib/index.js??ref--5-0!../../../build/md-loader/index.js!./bigData_hadoop_基础_1-1_HdfsShell.md?vue&type=template&id=6323f2a8&\""],"sourceRoot":""}